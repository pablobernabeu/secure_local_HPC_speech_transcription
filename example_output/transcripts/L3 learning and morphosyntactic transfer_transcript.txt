L3 learning and morphosyntactic transfer
========================================

Input file: L3 learning and morphosyntactic transfer.wav
Processed: 2025-11-10 23:02:51
Transcription system version: 1.0.0
Model: openai/whisper-large-v3
Audio enhanced before transcription
Language: English
Edited to remove likely spurious repetitions.
Privacy: Personal names masked using internal list (review recommended)
========================================

Welcome back to the Deep Dive. It's great to have you with us again as we, well, as we dig into some really fascinating science. It's good to be here. Today's topic is one that's pretty close to my own research interests, actually. We're exploring the cognitive science behind learning a third language. Right, L3 acquisition. And we've got quite a stack of material to unpack here. We do. And the core idea, really, is that learning your third, fourth, or, you know, nth language isn't just doing what you did for your second language all over again, but maybe a bit faster. The evidence is pretty clear now. The process is fundamentally different. Your brain's facing a whole new kind of challenge. Okay, so that's our mission for this deep dive then. We want to understand why it's so different and also how researchers are actually studying this. I gather they're using some pretty clever techniques like bespoke artificial languages and brain imaging. Exactly. Tools like EEG let us peek under the hood, so to speak, right from the very first moment someone's exposed to that new language. And through that, we can start to understand not just language learning, but maybe some bigger principles about how the human brain works. This idea of cognitive economy, for instance. The brain trying to be efficient, basically. Pretty much. It's always looking for the best return on investment, cognitively speaking. So we're going beyond just asking what gets transferred from, say, your first or second language when you start your third. We're asking how the brain chooses what to transfer and when it makes that choice, especially since everyone's different, right? Individual learners must tackle this differently. That's the core question. When you've got these two existing blueprints, your L1 grammar, your L2 grammar, which one does the brain lean on and why? Is it the languages themselves or is it you, the learner? Okay, let's get into it. Section one, the unique challenge of L3. My first thought, honestly, is that learning in L3 should be easier. You've done it before with L2. You know the ropes. You know how to study. So why does the research say the actual acquisition process is fundamentally different? What's the big deal with the initial conditions? It's a great question, and it highlights the difference between conscious learning strategies and subconscious acquisition mechanisms, the initial cognitive state is just vastly different. When you learn an L2, all the influence, all the transfer can only really come from one place, your native language, your L1. Right, only one prior system to draw from. Exactly. But the L3 learner starts with what we could call a multilingual toolkit. They don't just have more linguistic knowledge. They have it organized into at least two distinct grammatical systems, L1 and L2. And maybe even more crucially, their brain has already got practice managing competition between languages. They've been doing that L1, L2 juggle from the moment they start L2. That experience changes the game for L3. [Ah], okay. So it's not just having more tools. It's having the experience of choosing between tools or suppressing one to use another, which I guess makes the whole issue of cross-linguistic influence or transfer way more complicated for L3. It's not just L1 affecting the target language anymore. Precisely. The big question becomes, does the L3 learner transfer stuff from their L1 or their L2? Or maybe bits of both. Or maybe even all previously known languages, if they know more than two. And the research suggests the structure of those L1 and L2 languages really matters in predicting what happens. Systematically. There's a classic study from 2010 by Rothman and Cabrelli-aro that really nails this distinction. They looked at English native speakers learning French. Okay, English and French. Structurally, they're quite similar in some key ways, right? Like subjects. Exactly. Both are what we call non-null subject languages. You have to say the subject. She speaks French, not just speaks French. You can't drop the she. So for the L2 learners in their study, people who only knew English and we're learning French, this French rule was easy. It matched their English grammar perfectly. No mental gymnastics needed, or as the paper put it, no reconfiguration required for that specific feature. Makes sense. They just mapped their English structure onto French. Simple. But the interesting group was the L3 learners. That's where it gets fascinating. They also had English as their L1 and we're learning French as their L3. But crucially, their L2 was Spanish. And Spanish is a null subject language, right? You can say things like, Hablo espanol. Speak Spanish without the yo. Correct. So here's the puzzle. Their L1 English lined up perfectly with the target L3 French on the subject rule. The most efficient logical thing to do would be to transfer the English pattern. Use the L1 blueprint. They didn't. They didn't. Initially, these L3 learners' French showed clear signs of transferring the Spanish null subject property. They we're dropping subjects in French incorrectly as if French worked like Spanish. Wow. Okay, that is counterintuitive. Why would the brain choose the L2 Spanish rule, which caused an error they'd have to fix later when the L1 English rule was a perfect match for the target L3 French. That seems like extra work. It does seem uneconomical on the surface, and this is the core puzzle L3 research tries to solve. It forces us to think beyond just structural similarity. One idea is about activation. Maybe the L2, Spanish in this case, is in a higher state of activation because the bilingual brain is constantly working to manage two systems, or perhaps the L2 grammar, being less deeply ingrained or automatized than the L1, is somehow more available or visible to the learning mechanisms looking for patterns to borrow. So accessibility might trump structural perfection, at least initially. That's one strong hypothesis. Whatever the exact mechanism, the result was undeniable. The learning task itself was fundamentally different for the L3 group compared to the L2 group, even with the same L1 and the same target language. The L3 learners had this extra step of having to notice their mistake and unlearn the transferred Spanish feature. Which neatly brings us back to this idea of cognitive economy. Indeed. The seemingly odd transfer choice isn't just about language. It gives us a fantastic window into this general cognitive principle. The brain wants maximum results for minimum effort. In L3 learning, it's faced with multiple toolkits, L1, L2. Which one is the quickest, most accessible, maybe the most structurally close enough option to grab right now, even if it means a bit more tidying up later? So L3 acquisition becomes this natural experiment where we can watch the brain making these high stakes resource management decisions in real time. Fascinating. Right. And because that initial moment, what the learner brings and what they do right at the start seems so crucial, a lot of the L3 research, especially up until maybe 2015 or so, really zoomed in on these initial conditions. Trying to figure out what gets transferred first. Exactly. Yeah. And this focus led to basically two main camps, two competing ideas about how this initial transfer selection actually works. Okay. Lay them out for us. Camp one is the wholesale idea. Yeah, you could call it that. The most prominent model here is the typological primacy model, or PPM. The TPM argues for a full transfer of one of the existing grammars, either your L1 or your L2, really early on, almost immediately after you first encounter the L3. So it's like the brain decides, okay, this new language looks mostly like Spanish. Let's just copy the whole Spanish operating system over to get started. That's a good analogy. And the decision whether to copy L1 or L2 is supposedly driven by a quick overall assessment of typological similarity. Does the new L3 share more major structural features big picture stuff with l1 or l2 so the parser does a quick scan and says overall structure feels more like l2 and boom the entire l2 grammar gets activated as the foundation for the L3. That's the prediction of the TPM. One big decision based on global similarity sets the whole path. It's about simplifying the initial learning task by choosing one complete existing system. Swap the whole engine block, as you said earlier. Okay, that's one view. But there's another camp that thinks that's too simplistic, right? Yeah. Treating language like a single block you copy or don't copy. Exactly. The second group of theories, including things like the linguistic proximity model, LPM, and the scalpel model, SM, they really question whether that initial stage is so special and decisive. They argue that influence from L1 and L2 isn't a one-time decision. Instead, they suggest it's more about continuous activation of both L1 and L2 throughout the learning process. Okay, so it's not wholesale transfer, it's more granular. Precisely. These models predict that influence happens on a property-by-property basis. For any specific grammatical feature you encounter in the L3, say, word order or how plurals work. Your brain might be checking the rules in both L1 and L2 simultaneously. Like comparing individual parts, not the whole engine. Right. And the feature from either L1 or L2 that seems closest or most relevant to the L3 feature you're currently processing might exert influence in that moment for that specific property. So you could, theoretically, be influenced by your L2 Spanish for subject dropping, but by your L1 English for adjective placement, all within the same sentence or stage of learning. It's a constant negotiation, not a single initial choice. So the core debate is really about the nature of the transfer choice itself. Is it one big foundational decision based on overall similarity early on, TPM, or is it this ongoing, dynamic, feature-by-feature negotiation driven by local similarities? LPMSM. That's the crux of it. And figuring out which is closer to reality tells us a lot about how the brain manages complex, competing knowledge systems more generally. Is it about big picture efficiency or moment-to-moment optimization? Okay, so we have these competing theories, wholesale transfer versus granular continuous influence. But how do you actually test which one is right? Real world language learning is messy, extremely messy. And that's the methodological hurdle the field ran into. To really tease apart these models, you need incredible control over what the learner is exposed to, when, and how much. Which is basically impossible if you're just observing people learning French in Paris or Spanish in Madrid. Everyone's experience is different. Their L2 proficiency varies, their motivation, the type of input they get. Exactly. All that variability, that noise, makes it incredibly hard to get clear results, statistically speaking. You need large groups of learners who are as identical as possible in their background and their learning experience. A nightmare to find in the real world for specific language combinations. Pretty much. So the field needed a way to impose control. And that led to a really clever methodological shift. The artificial language, or AL paradigm. Researchers just make up languages. Essentially, yes. Miniature languages designed specifically for the experiment. It sounds a bit strange, but it offers huge advantages. Okay. What are the main benefits? Why go to the trouble of inventing a language benefit number one total control over the input you know for a fact the language is completely new to every participant nobody has any prior exposure and you control exactly what they hear, how much they hear, and in what order. Everyone gets the identical learning experience, which cleans up the data massively. Equal starting point for everyone. Okay, equal exposure. What else? Benefit two, custom design. You don't have to hope that, say, Basque and Finnish happen to have the exact grammatical contrast you need to test your theory. You can build it directly into your artificial language. You can design features that create really TaylorMade. Clever. And the third big advantage is sample size. Instead of searching for rare multilinguals, like, I don't know, native, turpish speakers who know fluent Japanese and are learning Quechua. Good luck finding 30 of those. Exactly. Instead, you can recruit large groups of relatively common bilinguals like Spanish-English speakers who are much easier to find. You bring them into the lab, teach them your AL, and you suddenly have the statistical power you need. And I read about this technique they use, the semi-artificial approach, using familiar words. [Ah], yes, that's a really neat trick. Instead of inventing totally new words, glim means cat, snork means run, which would take ages for participants to memorize. They'd spend all their time just learning vocabulary, not grammar. Right. So they often use familiar vocabulary, like basic English words, but embed them in a completely novel grammar, made-up endings, new word order rules, things like that. This bypasses what's called the lexico-semantic bottleneck. Participants can understand the basic meaning of the words, so they can focus their cognitive resources almost immediately on figuring out the new grammatical patterns. It lets researchers get straight to the syntax processing. OK. The control is undeniable. It allows for really elegant experimental designs. But let's play devil's advocate. How much can learning blickities in a lab for a few hours really tell us about the rich, messy process of learning a real language over years? What about ecological validity? That is the crucial caveat, and it's a completely valid concern. You absolutely cannot claim that learning an AL in a row perfectly replicates learning Spanish during a year abroad. The AL paradigm is powerful for isolating specific cognitive mechanisms. It shows us how the brain responds to structured linguistic input under controlled conditions, how it spots patterns, how it maybe makes those initial transfer decisions. But we have to be cautious. These findings likely reveal the initial cognitive steps, the precursor functions, the machinery starting up. They don't capture the whole complex, socially embedded, long-term journey. And actually, the early brainwave results really underscored this limitation. Okay, so we had these controlled AL experiments designed to see which prior language gets transferred, maybe L1 or L2. How do researchers actually measure that transfer happening in the brain, like moment by moment? This is where brain imaging comes in, specifically EEG electroencephalography. It lets us measure electrical activity in the brain with really fine-grained timing. We're looking for event-related potentials or ERPs. These are characteristic bumps or dips in the brainwave that occur in response to specific events, like hearing a word or, crucially, detecting a grammatical error. And there's a specific ERP component linked to grammar processing. Yes, the gold standard, especially for detecting syntactic errors in a way that seems automatic and native-like, is the P600 component. The P600. Remind us what that signifies. It's a positive voltage deflection, a P peaking around 600 milliseconds after the brain encounters a grammatical violation. Think of it as the brain's automatic syntax error flag. It suggests two key things. One, the brain implicitly detected something was wrong with the sentence structure, and two, it's trying to repair or reanalyze that structure. It's the signature of your internal grammar checker working automatically. OK. So if the TPM theory was right, if learners instantly transferred their whole L1 or L2 grammar to the new artificial language. Then when we present them with a sentence in the AL that violates a rule from that supposedly transferred grammar, their brain should automatically generate a P600 response right from the get-go. It would show that transferred system was actively and implicitly processing the new input. That makes sense. The P600 would be the smoking gun for immediate automatic wholesale transfer. But that's not what the early studies found, was it? I'm thinking that Gonzalez-Alonzo paper from 2020. No, it wasn't. And it was a big surprise, actually. After relatively short exposure times in the lab, we're talking around 45 minutes of training on these ALs, the participants did not show the expected P600 when they encountered grammatical errors? No P600. So no sign of that automatic implicit grammar checker firing. Not reliably, no. It suggested that whatever transfer might be happening, it hadn't yet reached the level of automatic, implicit grammatical processing. The system wasn't running on autopilot yet. So if the brain wasn't giving the automatic language error signal, what signal was it giving? This was the really interesting part. For certain types of errors, they looked specifically at violations of grammatical gender rules in an AL designed to mimic Spanish gender. They didn't see a P600, but they saw something else. A P300-like effect. P300, not P600. How is the P300 different? What does that signal tell us? It's fundamentally different. The P300 isn't specific to language at all. It's what we call a domain general component. You can elicit a P300 with all sorts of unexpected or attention-grabbing stimuli, like seeing a rare target in the sequence or hearing an oddball sound, it primarily reflects things like attentional resource allocation and the detection of unexpected or task-relevant events. It's basically the brain saying, whoa, hang on, something unexpected happened there. Pay attention. I need to consciously figure this out. Okay, that's a crucial difference. So exposure to the L3, processing grammatical errors isn't yet an automatic subconscious linguistic process. It's more of a conscious, effortful act of learning, noticing discrepancies, allocating attention, trying to make sense of the input. So the brain is treating it more like solving a puzzle than speaking a language at that point. That's a good way to put it. The P300 findings suggested that the cognitive work happening initially is more about domain general functions. Attention, pattern detection, maybe working memory laying the groundwork before the specialized linguistic system, the one that would produce a P600, can fully engage and automate. Which really forces a rethink of the timeline, doesn't it? It implies that the kind of transfer the models we're talking about maybe doesn't happen instantaneously. It needs time to consolidate. Precisely. The short exposure times typical in those initial AL lab studies, while great for control, we're likely just too short to capture the transition from this effortful P300 stage to the more automatic P600 stage. So the conclusion was you need to watch learners for much longer. Yes. To truly see if and when linguistic transfer happens implicitly and how it develops you need to track learners over extended periods this realization really pushed the field towards designing much more ambitious longitudinal studies okay so the short one-shot lab experiments gave us the P300. Insight initial learning is effortful attention, but couldn't show us the consolidation into automatic P600 processing. The logical next step, track people over time. Exactly. And that led to the design of this much larger, more complex, multi-site longitudinal study that's currently underway. It's a big undertaking. They're running parallel experiments in Madrid with Spanish-English bilinguals and in Tomsi in Norway with Norwegian-English bilinguals. The goal is to watch that P300 to P600 transition happen, hopefully over several months of controlled exposure. Wow, OK. Running it in multiple sites with different language pairings must give much richer data. How is it actually structured? How do you maintain control over months? It's structured across six distinct sessions. Session one is entirely online, and it's all about gathering baseline data. Participants fill out a very detailed language history questionnaire, the LHQ-3, so the researchers know exactly their background in L1 and L2, and crucially, they also complete a whole battery of cognitive tests before any training starts. Measuring things like memory and attention from the outset. Got it. What happens next? Then come the core lab sessions, sessions two, three, and four. These are where the learning and the main EEG testing happen. In each session, participants get incremental, implicit training on grammatical properties of the artificial language. It's designed to feel more like exposure than explicit rule memorization. And immediately after the training block, they do an ERP experiment testing their brain responses to violations of those properties. And is the training cumulative? Do they keep building on what they learned before? Yes, that's key. The design ensures cumulative exposure. For example, filler sentences used in session two might get replaced in session three with sentences that correctly use the grammar point taught in session two. This constantly reinforces the previously learned material while introducing new stuff, trying to mimic how real language exposure builds over time. Okay, that sounds intense. What about the later sessions? Session 5 is back online. Here, they re-administer that same cognitive battery from session one. This is to see if the intensive language training itself has potentially changed the participant's baseline cognitive abilities. [Ah], the bidirectional effect does language learning boost cognition? We'll come back to that. What's the final session? Session six is the crucial follow-up. It happens back in the lab, but after a significant delay, about four months, during which the participants get no further exposure to the artificial language at all. A test of consolidation then. Did the learning stick? Precisely. This session measures the relative consolidation and stability of the knowledge they acquired? Did the brain responses, maybe the P600 if it emerged, remain stable after a long break without practice? Or did things decay? Did it truly become implicit long-term knowledge. That structure seems really well designed to capture the dynamics over time. What about the language itself? What grammatical features did they build into this AL to tug the transfer theories. They carefully chose three grammatical properties designed to create interesting contrasts based on the participants' L1 and L2 backgrounds, Spanish-English versus Norwegian-English. Okay, what's the first one? First is gender agreement. Grammatical gender exists in both Spanish and Norwegian, but not in English. So both bilingual groups have some prior experience with gender. However, the way gender works in the artificial language is deliberately different from how it works in either Spanish or Norwegian. This tests how learners transfer and adapt an existing but not perfectly matching feature. Okay, so it's familiar territory but with a twist. What's next? Second is differential object marking, or DOM. This is a feature where you mark direct objects differently depending on factors like whether they're animate or definite. Spanish has this, think of the personal A. Right. Exactly. But Norwegian and English don't have DOM, so for the Spanish-English group, DOM in the AL is something they might transfer from their L1 Spanish. For the Norwegian-English group, it's completely novel. [Ah], that creates a clean comparison. Transfer versus learning from scratch for the same feature. Very neat. And the third property. The third one is designed to be novel for everyone. Object-verb number agreement, OVA. This means the verb has to agree in number, singular-plural, with the object of the sentence, not just the subject. This specific type of agreement doesn't exist in English, Spanish, or Norwegian. So for all participants, learning OVA is purely about extracting a brand new grammatical pattern from the input with no possibility of direct transfer from their existing languages. Okay, so you have one transfer of a familiar but different feature, gender. Two, potential transfer for one group versus new learning for another, DOM. And three, totally new pattern learning for everyone, OVA. Yeah. That covers a lot of ground. Exactly. And by introducing and testing these properties at different points across sessions two, three, and four, they also manipulate the amount of exposure each property gets. This lets them track how processing the potential shift from P300 to P600 changes with increasing input for both potentially transferred and entirely new rules. It sounds incredibly complex to run, and I imagine keeping participants engaged for six sessions over several months, including EEG, must be tough. Attrition must be a worry. Huge worry. Longitudinal studies, especially demanding ones like this, always face high dropout rates. Thank you. To even proceed to the ERP part for that session. And frankly, they also use significant financial compensation to motivate participants to stick with it. It just highlights how challenging, but also how valuable this kind of detailed longitudinal data is for understanding the real dynamics of L3 acquisition. Okay, so this big longitudinal study is tracking the process over time. But you mentioned earlier that L3 research historically hasn't paid much attention to individual learners and their cognitive differences. Is this new study addressing that? Yes, thankfully. It's tackling what has been called a glaring disregard for individual differences in much of the prior L3 work, which is strange because we know from L2 research and bilingualism studies that factors like memory, attention, and inhibition play a huge role. So how are they bringing the individual learner into the picture? By systematically measuring key cognitive abilities specifically, executive functions both at the very beginning, in session one before before training and again towards the end in session five after most of the training. Right, those cognitive batteries you mentioned. What specific functions are they focusing on? They're targeting three main areas thought to be highly relevant for managing multiple languages. First, inhibitory control or conflict resolution. The ability to suppress irrelevant information or responses. Exactly. They measure this using the classic Stroop task, where you have to name the ink color of a written word, like the word B-L-U, printed in red ink, and resist the urge to just read the word B-L-U-E. Which seems directly relevant to language switching, right? Suppressing your L1 while speaking L2, or managing interference between L1 and L2 when learning L3. Precisely. Better inhibitory control might make you better at keeping the language systems distinct and avoiding unwanted intrusions or transfer. Okay. Inhibition. What's the second factor? Working memory capacity. This is measured using tasks like the digit span, DGS, where you have to hold and repeat back increasingly long strings of numbers. Working memory is crucial for holding information online while you process it essential for understanding complex sentences, integrating new information, and potentially keeping representations from both L1 and L2 active simultaneously during L3 learning. Makes sense. More mental workspace helps juggle complex input. And the third one. The third is implicit learning aptitude. This goes beyond conscious memory and taps into your ability to pick up patterns subconsciously. They use the alternating Reaction Time, ASRT, task. It's a reaction time task where hidden patterns are embedded in the sequence of stimuli. People who are good implicit learners get faster at responding to the pattern sequences compared to random ones, often without even realizing there is a pattern. [Ah], so this measures your knack for automatically extracting statistical regularities from the environment, which sounds exactly like what you need to do to internalize grammar rules without being explicitly taught them, like moving from P300 to P600. Exactly. It taps into that procedural learning system. Okay. So they measure inhibition, working memory, and implicit learning before and after. What's the main hypothesis? Just that people with better scores learn the AL faster or better? That's part of it, sure. They expect these factors to predict overall learning success. But the really interesting idea is what they call the modulator hypothesis. The idea isn't just that cognition predicts how well you learn, but that it might actually influence how you learn specifically, it might modulate the source of transfer. Whoa, okay. Unpack that. How could my working memory score affect whether I transfer from my L1 or my L2. Well, imagine managing and comparing two complex grammatical systems, L1 and L2. To decide which bits are most useful for the L3 is cognitively demanding. It requires resources. So perhaps learners with very high working memory capacity can afford to engage in that complex comparison and selectively transfer features from whichever language is structurally closer, maybe even the more complex L2. Conversely, someone with lower working memory capacity might default to a simpler strategy, perhaps relying more heavily on the more automatic, less resource-intensive L1, or maybe transferring more wholesale from one source without fine-grained comparison simply because that's less demanding. That's fascinating. So your individual cognitive profile could actually shape your personal pattern of cross-linguistic influence. It's not just about the languages. It's about the brain processing them. That's the exciting possibility they're investigating. It really individualizes the process of transfer. And then there's the other direction and the bidirectional relationship. Does learning the language change your brain? Yes. That's why they test cognition both before session one and after session five. The intense process of learning and managing this new linguistic system, juggling it with L1 and L2, could potentially exercise and strengthen certain executive functions. It's cognitively demanding work, so they might see improvements in, say, strip performance or working memory capacity as a direct result of the language training itself. Like cognitive cross-training? Sort of, yes. And they're even looking at this with resting state EEG data collected before and after the main training period to see if there are changes in baseline brain activity patterns that might reflect altered cognitive functioning due to the L3 learning experience. This whole approach seems to really bridge the gap between hardcore linguistics and broader cognitive science. Absolutely. It reframes L3 acquisition and transfer not just as a linguistic phenomenon, but as a prime example of how the human mind applies general principles of learning, memory, attention, and resource management when faced with complex new information. It's using L3 learning as a test case for cognitive economy in action. Hashtag tag outro. Wow, okay. That was definitely a deep dive. Let's try and pull the main threads together. We started by establishing that learning a third language, L3, isn't just L2 learning on repeat. It's fundamentally different because the learner starts with multiple linguistic systems and experience managing them. Right, and this leads to complex patterns of cross-linguistic influence, where the brain has to choose whether to borrow from L1, L2, or perhaps both? Early attempts to study this using artificial languages and EEG, suggested something surprising. The very initial stages seemed dominated by conscious effort and attention marked by that P300 brain response, rather than the automatic, implicit chromatical processing we associate with fluency, which would show up as a P600. Exactly. That finding really highlighted the need for longer-term studies to track how that initial, effortful learning potentially transitions into more automatic implicit acquisition over time which brings us to this large-scale longitudinal study currently underway It's designed to watch that p300 to p600 shift happen using carefully crafted artificial languages and tracking learners over months. And crucially, it's integrating individual differences, measuring cognitive functions like working memory and inhibition, not just to predict how well people learn, but to test the fascinating idea that these cognitive profiles might actually influence which prior language, L1 or L2, the brain chooses to rely on for transfer. So here's the really provocative thought to leave you with today. This research is pushing towards a model where your fundamental cognitive abilities, your baseline scores on things like memory span or the Stroop test might do more than just set your speed limit for learning a third language. They might actually determine the strategy your brain employs. They could be the deciding factor in whether your brain leans on your native tongue or your second language when figuring out the new one. Your individual cognitive makeup could literally dictate the source code for your developing L3 grammar. It really underscores the incredibly intricate dance between language, cognition, and the individual mind. It certainly does. There's still so much to uncover. Absolutely. Well, that's all we have time for on this deep dive. Thanks for joining us. My pleasure. Until next time, keep exploring the amazing world inside your head.

