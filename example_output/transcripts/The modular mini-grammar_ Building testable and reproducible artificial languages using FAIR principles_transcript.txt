The modular mini-grammar: Building testable and reproducible artificial languages using FAIR principles
=======================================================================================================

Input file: The modular mini-grammar_ Building testable and reproducible artificial languages using FAIR principles.wav
Processed: 2025-11-10 23:12:04
Transcription system version: 1.0.0
Model: openai/whisper-large-v3
Audio enhanced before transcription
Language: English
Edited to remove likely spurious repetitions.
Privacy: Personal names masked using internal list (review recommended)
=======================================================================================================

Welcome back to the Deep Dive. It's great to have you with us again. Hello, everyone. Today, we're tackling something really foundational. It's maybe not the flashy headline stuff, but it's absolutely vital for, well, for science actually working properly. We're talking about methodology. [Mm]-[hmm]. The engine room, as they say. You know, we often focus on the big discovery, the finding. But people like Barcelou pointed out years ago, the methods you use and the findings you get, they're completely tied together. Like two sides of the same coin. Exactly. If your methods are shaky, if that foundation is built on sand, the whole thing, the finding, it becomes unstable. So we're looking at a move away from sort of making it up as you go along, those ad hoc methods, towards setting some, I guess, minimum standards for how we prepare research materials. That's the core idea, establishing robust systems. Right. And that's our mission for this deep dive. We're going to explore a really advanced, quite rigorous methodology. It's developed by Dr. [NAME] [SURNAME] and his colleagues. They're based at UIT, the Arctic University of Norway, and also Nebrija University in Spain. They've essentially laid out a blueprint specifically for artificial language studies in cognitive science, but the principles apply much wider, I think. It shows what truly solid, reliable research material preparation actually looks like. And we're going to trace how they build it, right? Yeah. A fully traceable, reproducible system is a big step beyond, you know, just passing around potentially error-filled documents. Even beyond the static Excel sheet, maybe. Definitely beyond that. And, you know, this whole push isn't just academic navel-gazing. It comes from a real need. The reproducibility crisis in science, well, it's been under the microscope for, what, over a decade now? Yeah, seems like it. When researchers like Nosek and his team back in 2012 and then the Open Science Collaboration a few years later, when they actually tried to replicate published findings, the results we're, well, let's just say modest. Yeah. Often alarmingly so. Right. It kind of pulled back the curtain on how gradual some common practices might be. It really did. It showed some hidden weaknesses. There's that great Latin phrase, the source material used, which I think captures the urgency perfectly. It's about making mistakes versus sticking with them. [Ah], yes. Errere humanimist perseverar autum diabolicum. Exactly. Which means. To err is human, but to persist in error is diabolical, or maybe less dramatically. There's just no good reason to keep making the same avoidable systematic mistakes if we know how to do better. And that really applies to fields like artificial language studies, doesn't it? Where you're building these complex, carefully balanced lists of sentences or words. Absolutely. Those stimuli have to be constructed so precisely. When you look across the field, the quality and accessibility of those materials, it's all over the place. A lot of it doesn't meet even basic standards. OK, so let's define that spectrum a bit. What do these different levels of quality or accessibility look like in practice the study, the specific sentences, the audio files, whatever was used, they're just not directly available anywhere. So you read the paper. It sounds interesting, but you hit a wall if you want to check it or build on it. Precisely. You can't access the core components. The source points out this leaves the reader in a really difficult position. You saw this in some earlier studies like Consuelo Alonso et al. Or [SURNAME] Short et al. The materials just weren't shared. Okay, that's clearly a problem. What about medium? That sounds like an improvement, but maybe not the whole story. Yeah, medium can be a bit deceptive. You might get some materials, maybe the final list of stimuli used in the experiment. [Ah], so you can see the end product. Right. But you can't see how it was made. Right. The workflow, the actual steps, the calculations, the scripts used to generate those materials, that's missing or it's completely unreproducible. Maybe it was done manually in Excel, maybe it was some undocumented script. So you can't trace the origins, you can't check the process for errors. Exactly. You get the output, but the input and the transformations are opaque. Studies like Cross et al. Or Mitropanova et al. Might fall here as some material shared, but the process isn't transparent or repeatable. Which leads us to the goal, the high standard. What does that involve? High is where Burnabew and colleagues are aiming. It means both the final materials and the entire workflow used to create them are reproducible. And crucially, it's done using a modular framework. Modular, meaning broken down into pieces. Yes, based on minimal components that can be understood, tested, and importantly, modified or expanded by anyone else. That's the gold standard. It makes the research truly testable and extendable. Okay, so if that's the target, this high standard where our future selves or other researchers can easily reproduce, test, and even build upon the work, we need some kind of shared understanding, some principles for what that actually looks like in practice. And that's where the FAIR guiding principles come in. F-A. F-A-I-R. I've heard that acronym a lot recently. It came from the wider data management world, didn't it? That's right. Wilkinson and colleagues laid it out back in 2016. It's essentially become a widely adopted protocol for managing scientific data and materials properly. Think of it as a checklist for ensuring scientific longevity and openness. Let's break it down then. F-A-I-R. What does the F stand for? Findable. Findable, yeah. And this seems, you know, obvious at first glance. Of course, things should be findable. But surely online, isn't everything findable now? Well, findable in the fair sense means more than just maybe mentioning it in a paper. It means the materials themselves must be deposited in a public, stable, well-known online repository. Like the Open Science Framework, OSF, or're putting the reader in a terrible position. They're forced into what the paper calls the tribulation of having to assume one of two things, either that your complex material creation process had absolutely zero errors, which is, let's be honest, unlikely, or that it did have errors, but of an uncertain degree. [Ah], so you're forcing them to guess or just trust blindly. Right. Public sharing removes that burden, that uncertainty. It's presented as a fundamental condition, almost a prerequisite for the rest. That really reframes it. It's not just nice to share. It's about intellectual honesty and relieving that epistemological burden from the reader. Okay, so F is findable. What's A? Accessible. Accessible, yes. So findable means you can locate the materials. Accessible means you can actually use them once you find them. Use them how? Like open the files? Open them, understand them, run them. This principle strongly implies using open standards and crucially open source software. [Ah], okay. So if your workflow relies on some super expensive proprietary software, then it's not truly accessible, is it? You've put up a huge financial barrier for students, researchers in less wealthy institutions, or even just independent scholars. Accessibility means anyone, any potential stakeholder, should be able to engage with the materials. Makes sense. No paywalls for the fundamental building blocks of science. Okay, FAI is interoperable. That sounds a bit more technical. What does interoperability mean for, say, artificial language stimuli? Interoperable means the materials are structured so they can work with different systems or be easily integrated into other workflows. Think plug and play. In this specific workflow we're discussing, it's achieved through modularity instead of one giant monolithic script or file that does everything you break it down yes you break the language the stimuli down into simple distinct base components little building blocks these smaller well-defined parts can then be combined in different ways may be used with a different experimental platform later or adapted for a new study without breaking the whole system so it's flexible adapt adaptable, highly adaptable. That structure ensures that components can talk to other systems or workflows more easily. And that leads nicely into R, which is reusable. How is that different from just being able to reproduce the original study? Reusability is broader. It's about the long-term scientific value. It means someone else can take your components, your base word lists, your grammar rules, your documented procedures, your scripts, and not just replicate, but modify them, adapt them, repurpose them for totally new questions. [Ah], so maybe testing a different linguistic feature using the same basic structure. Exactly. Or maybe adapting the task for a clinical population. Because the components are modular and the process is transparent, this kind of reuse becomes feasible, rather than requiring someone to reinvent the wheel completely from scratch every single time. So F-A-A-R, findable, accessible, interoperable, reusable, and that starting point, sharing publicly online. That's the conditio sine qua non, the absolute essential condition, as the paper puts it. Without findability through public sharing, the other three, accessibility, interoperability, reusability, they kind of lose their power. You can't access or reuse what you can't find. Okay, let's pivot now and really get into the weeds of this R-based workflow that [SURNAME] and colleagues developed. This is the practical implementation, the how-to for achieving that high standard of reproducibility we talked about. Right, the nuts and bolts. When we think about older methods, maybe using Excel or even just basic text files to create stimuli lists. What was the fundamental weakness there? And how does using a scripting language like R change the game? The fundamental weakness of static documents like Excel is opacity, lack of traceability. Imagine you're making a list of, say, 500 carefully counterbalanced sentences. In Excel, you might be copying, pasting, maybe manually sorting, deleting rows, perhaps running a quick formula here and there. If you make a tiny slip-up copy of the wrong column, delete one row too many. It's baked in. And you might not even know. Exactly. Yeah. That error is now permanently in your final stimulus list. And crucially, there's often no record of how it got there. No history tracking that specific manual change you made three weeks ago. Right. No audit trail. None whatsoever. Yeah. So the shift to using R isn't just about fancy coding. It's about replacing that opaque, error-prone manual manipulation with transparent, traceable computation. So R isn't just for the stats at the end. They're using it for the creation process, too. Absolutely. For preparing everything. The texts, yes, but also potentially images, audio timing lists, anything that goes into the experiment. The huge advantage of scripting this entire preparation process- Is the traceability. Yes, it acts like a meticulous record, almost like a video recording, as you suggested earlier. Every single step, every addition, every filter, every randomization, every constraint check is explicitly written down in the code. So it forms a kind of dot map of how the materials came to be. A perfect map. Technically, it defines a directed acyclic graph, or DAG, for material creation. If a researcher spent months crafting these materials, the script is the documentation of that process. And if an error is found later. You can pinpoint exactly which line of code caused it. Precisely. You trace it back. This takes a massive burden off human memory and makes the whole process verifiable. It offloads that huge documentation effort under the code itself. That makes a lot of sense. But playing devil's advocate for a second, R can have a steep learning curve for some researchers, right? Does relying on a specialized language like R potentially create a new kind of barrier? Does it conflict with the A for accessible principle we just discussed? That's a really fair point and something the community definitely discusses. Is the tool itself accessible? Yeah. But I think the argument here, and I tend to agree, is that using R actually enhances accessibility in the long run for several reasons. First, R is completely free and open source. OK, no financial barrier, unlike some commercial stats packages or experiment builders. Right. Second, R is arguably the dominant language, the lingua franca, for statistical analysis and data visualization in many scientific fields, including psychology and linguistics. So many researchers either already have some familiarity or will need to learn it anyway for analysis. So it keeps the whole pipeline from creation to analysis in one environment, potentially. Potentially, yes. It streamlines things. Python is also very popular, especially for running experiments, as we'll see. But R has this huge stats community and infrastructure. And the key is, once the R script for preparation is written, even if it was complex to write, it can be run by anyone, anywhere, with R installed. [Ah]. So the complexity is in the creation, but the execution, the reproducibility, is made simpler for others. Exactly. It front-loads the effort to ensure downstream reproducibility and accessibility. Okay, let's look closer at how they achieved this high-level reproducibility using R. You mentioned modularity earlier. How does that work in terms of the actual files and scripts? If the script is the video recording, what does the filing cabinet look like? It's very organized, built around this idea of minimal components. If you we're to look inside their project structure, in the folder for stimulus preparation, you wouldn't just find one massive final file. Instead, you find these core base files. Base files. Yeah, things like Norway site, basestimuli. Csv, or Spain site, basestimuli. Csv. These files don't contain the final complex sentences ready for the experiment. What's in them then? The absolute simplest building blocks. The raw ingredients, vocabulary items, maybe grammatical markers, rules, constraints that define the artificial language they're working with, just the fundamental pieces. Can you give an example? What would a minimal component look like in practice, let's say, for verbs? Okay, sure. So instead of generating a whole sentence like, the girl remembered the tree directly in the base file, you define the verb types and their properties separately. You might have a line defining a component like copulab and it's possible forms, maybe is and are then another component for transitive verbs listing maybe remembered a forgot a saw a where a signifies it needs an object the definitions are kept really simple and abstract in these base files. And the benefit of keeping them so simple? Flexibility and maintainability. Let's say two years later they want to run a follow-up study but use a different set of transitive verbs. They don't have to untangle some giant complex script or final stimuli list. They just edit that small base component file for transitive verbs. Exactly. Swap out the words in that one simple file. The rest of the system, the scripts that assemble the sentences, can remain largely unchanged. It makes updates and modifications much, much easier and less error-prone. It adheres to the principle of parsimony, keeping things as simple as possible. Okay, so you have these simple modular base components. How do they get assembled into the final complex stimuli needed for the experiment, ensuring all the counterbalancing and constraints are met. That's where the traceable assembly happens, driven by the R scripts. All the final stimuli lists are generated by running one main script. It acts as the single entry point. Logically, they called it something like compilial stimuli. One master script to rule them all? Sort of, yes, but to avoid that master script becoming, you know, thousands of lines long and completely unreadable. Which would defeat the purpose of clarity. Absolutely. They use a nested structure. That master script doesn't contain all the code itself. Instead, it sources or calls smaller, more focused scripts. Like chapters in a book, as you said. Exactly like chapters. So maybe for each experimental session or each specific type of stimulus, there is a dedicated script, like Session 2 Compilio Stimuli. R or Training Stimuli Compile. And the master script just calls these in the right order? Precisely. The master script acts like an orchestrator, calling these Nessun scripts sequentially. This keeps everything tidy, modular, and much easier to understand and debug. If there's an issue with the Session 2 stimuli, you know exactly which smaller script file to look into. That hierarchical structure sounds key for maintaining both clarity and the interoperability and reusability we talked about with FAIR. It's absolutely crucial for making the whole system manageable and robust. Now, this is where it gets really impressive for me. The built-in automated testing. It's one thing to script the process so it's traceable, but how do they actually check the quality of the generated stimuli during the creation process to prevent those spurious effects the source mentions? This is a real hallmark of a mature, rigorous workflow. They don't just rely on manual checking at the very end. They build quality control checks directly into the R scripts themselves. So the code polices itself, in a way. You could put it that way, yes. The script doesn't just generate, it validates as it goes. Let's look at that specific example they provided snippet two in the paper about checking the frequency of occurrence for certain features. Does that work how does the code verify say counterbalancing okay so the script targets specific columns in the data frames being generated columns that are critical for the experimental design. Things like maybe the gender of a noun, the noun gender, or the grammatical number, or the specific verb being used. Stuff you need to make sure appears equally often or in specific ratios. Exactly. The goal is perfect balance or controlled imbalance if the design requires it. So for a simple check, like ensuring masculine and feminine nouns appear equally often overall. The R code does something clever. It looks at all the values in that noun one gender column. Okay. It counts how many times each unique value, masculine, feminine, appears. Then it checks how many frequency counts there are. Unique frequency counts. Explain that. Right. So if you have exactly 50 masculine nouns and exactly 50 feminine nouns, the frequency count for masculine is 50, and and count for feminine is also 50. There's only one unique frequency value, 50. [Ah], I see. The number of times they appear is the same. Yes. But let's imagine a bug in the script accidentally generated 80 masculine nouns and only 20 feminine nouns. Now, the frequency for masculine is 80 and the frequency for feminine is 20. Those are two different frequency counts. So the number of unique frequencies is two, 80 and 20, not one. Precisely. The code checks. Is the number of unique frequency counts exactly equal to one? If yes, great. That specific check passes everything in that column appears equally often. If the number is not one. Alarm bells. Yes. The workflow is designed to immediately issue a warning message right there on the console output. Something like, warning, some elements in column noun one gender appear more often than others. That's brilliant. It forces the researcher to stop and fix the script right then and there rather than discovering the imbalance potentially months later during data analysis. Exactly. It catches what the paper calls bleakening disparities automatically, saving potentially weeks of wasted effort or, worse, contaminated results. But the researchers, true to the careful tone, add an important caveat about this automated check, don't they? They do, and it's important. They are very measured. They acknowledge that this kind of simple frequency check is essential for catching obvious errors. But it's not foolproof. It can't verify every complex constraint. Like what, for example? Maybe complex rotational logic, ensuring specific words appear equally often in, say, subject versus object position across different conditions or sessions. Those more intricate dependencies might still require a careful manual inspection of the final output tables generated by the script. So the automated tests are like a powerful first line of defense, but not a replacement for final human oversight? Exactly. They provide an essential safety net against gross errors, but they don't eliminate the need for careful review. It shows a realistic, appropriately skeptical approach to automation. OK, so the stimuli are prepared rigorously using R. Now let's look at actually running the experiment. They chose OpenSesame for this part, alongside R. That choice seems to reinforce the commitment to open source, back to the fair principle specifically accessible absolutely the combination of R for preparation and open sesame for experimental delivery is a very deliberate open source pairing. R, as we said, is free and has a massive global user community. If you run into an issue with an R script, chances are someone online has faced it before. And OpenSesame. OpenSesame is also free and open source. It's a very capable graphical experiment builder, popular in psychology and cognitive science. While it's user community might be smaller than R's massive base, it's still substantial and very active, especially for common experimental tasks. There are forums, documentation. And choosing open source tools like these avoids dependency on private companies. That's a huge factor. If you build your entire experimental paradigm using proprietary software, say E-Prime or Presentation, you're tying the future accessibility and reusability of your work to that company's decisions. Right, if they suddenly hike up license fees or discontinue the software. Your materials might become unusable for large parts of the scientific community overnight, or at least much harder to access. Sticking with well-supported open-source tools like R and OpenSesame really protects the long-term viability and fairness of the research. Okay, let's get practical. Running these kinds of cognitive science experiments, especially with artificial languages where every participant might get a slightly different version to counterbalance learning effects, requires careful participant-specific setup. How did this workflow handle that customization smoothly but securely? They used a nice trick involving custom Python code embedded within the OpenSesame experiment. OpenSesame allows you to insert Python script snippets quite easily. The goal is to make the experiment automatically adjust itself based on the participant's assigned number. They created a separate parameters file, a simple CSV file, stored outside the main experiment script something like Norway site parameters per participant CSV and that file lists all the participants in their specific settings exactly Exactly. Each row corresponds to a participant I'd, and the columns specify all the variables needed for that specific person. For example, which version of the artificial mini language they should learn, var dot language, or maybe the order of resting state blocks, var dot resting state order, or which specific stimulus list file they should be presented with. So when the experiment starts, the experimenter just types in the subject number. Right. And then, as shown in snippet three in the paper, the custom Python code kicks in. It reads the subject number the experimenter entered, var. SubjectNarr. It then uses that number to look up the correct row in that external parameter CSV file. And pulls in all the right settings for that specific person. Precisely. It automatically loads all those crucial variables, var dot language, var dot stimulus list, etc., into the OpenSesame environment for that session. It's seamless for the experimenter, but ensures all the careful counterbalancing done back in the R preparation phase is perfectly executed during the actual experiment. That's very elegant. And you mentioned security. There's a check built in too. Yes, which again shows their meticulous approach. To prevent someone accidentally running the experiment with a typo in the subject number, or maybe running an unassigned test participant, they put a constraint in the code. What kind of constraint? The system is hard-coded to only accept participant IDs within a specific predefined range that matches the parameter file. The source explicitly mentions IDs 1 through 1 through 144 for their study. So if you try to enter subject 145, the Open Sesame session simply won't start. It prevents accidental runs with invalid IDs, ensuring that data is only collected from participants whose parameters have been properly defined and counterbalanced in advance within that external CSV file. It guarantees data integrity. Okay, this workflow is used for an EEG study measuring event-related potentials, or ERPs. Now, in brain research like this, timing is absolutely critical. We're talking milliseconds. How did they ensure the ERP measurements we're precisely time locked to the appearance of the key words or stimuli? Yeah, this is super critical for eager P work. You need to send a precise timestamp or trigger to the EEG recording computer at the exact moment the event of interest happens on the participant's screen. Like the onset of a specific word in a sentence. Exactly. To achieve this, they again used custom Python code within OpenSesame. They wrote a specific function, which they called sendTrigger. Okay, and what does Send Trigger do? It's job is to communicate with the EEG recording hardware, typically through a serial port connection. When OpenSesame presents the critical stimulus, say, the target word appears, the send trigger function is called immediately. And it sends a signal. It does several things in sequence very quickly. First, it opens the connection to the serial port that the EEG system, like the brain vision recorder I mentioned, is listening on. Second, it writes the specific trigger code, just a number representing that event type to the port. Like event type 120 just happened. Precisely. But here's a crucial detail, highlighted in snippet 4 of their paper. The function doesn't just send the trigger and move on. It incorporates a very specific hardware-mandated delay. A delay? Why? Based on the BrainVision recorder manual, the function waits for exactly 10 milliseconds after sending the trigger code. This short pause ensures the EEG hardware has enough time to reliably register the incoming trigger signal before anything else happens. [Ah], okay. A little handshake time. Kind of, yeah. And then immediately after that 10 millisecond wait, the function does one more vital thing it resets the port by sending trigger code zero why reset it why send zero this is absolutely critical for clean ERP data if If the trigger signal, say code 120, just stayed active indefinitely, the EEG system wouldn't know when the next distinct event started. It would blur events together. Sending trigger zero immediately after the 10 meters delay So it creates sharp, distinct time markers for each critical event. Exactly. Sharp, unambiguous markers. And the trigger codes themselves we're quite specific too, weren't they? Not just one code for sentence start. No. Very granular. They use different codes for different phases. For example, codes 10, 11, 12, 13 might mark the beginning and end of the eyes-open or eyes-closed resting state periods. Code 5 might signal the appearance of the fixation cross before a trial. And for the actual language stimuli. They used a specific range. The paper mentions 110 through 253. And crucially, these triggers weren't just sent at the start of the sentence. They we're precisely time-locked to the onset of the specific word of interest within that sentence. This level of temporal precision is essential for linking the brain's electrical activity directly to the cognitive process being studied at that exact moment. This level of technical rigor is fantastic, but it brings us to maybe the unsung hero of reproducible science? Documentation. All this complex scripting and setup is useless if no one else or even your future self can understand it. Absolutely. Documentation is often treated as an afterthought, a chore to do at the end. But this workflow elevates it to a core component, essential for combating what's known as the curse of knowledge. The curse of knowledge. Yeah, it's the cognitive bias where once you know something well, it becomes incredibly difficult to imagine not knowing it when you've spent months building a complex system. It seems obvious to you how it works. Exactly. It feels self-explanatory. But to an outsider, a new student joining the lab, a collaborator, a reviewer trying to assess the work, or even yourself six months later, it can be completely opaque, bewildering even. So the antidote is generous documentation, as they call it. Generous documentation. Yeah. Not just minimal comments in the code, but clear explanations of the why and how throughout the project. And practically, how did they implement this? They used a very simple but effective approach, standard ReDKA files, but crucially, using the. Txt file format. Why. Txt specifically, not Markdown or something fancier? For maximum cross-platform compatibility and longevity. A simple. Txt file can be opened and read correctly on virtually any computer, now and likely decades from now, without needing special software. It maximizes accessibility. And these ReaNami files weren't just in the top-level folder, they we're placed strategically throughout the project directory, explaining the purpose of specific folders or scripts. That makes sense. Distributed documentation. What's really striking, though, is how the documentation went beyond just explaining the code. It also meticulously detailed the human procedures involved in running the experiment in the lab. [Ah], documenting the wetware, not just the software. Exactly. This shows a really holistic view of reproducibility. The entire process, human and computer, needs to be documented to be truly fair. Can you give some examples of that procedural documentation? What kind of human steps did they detail? Oh, quite a few. For instance, they specified how the experimenters should maintain an instantly updated online session logbook. This ensures everyone on the team has real-time visibility into which participant is currently running, what session they're on, and any issues encountered. Okay, so shared real-time tracking. Yes. They also detailed the exact sequence for launching the experiment software. You must open the OpenSesame program first and then use the File Open menu to load the specific experiment file. Why so specific? Because apparently, double-clicking the experiment file directly sometimes launches a separate background Python process. If that background window gets accidentally closed, you can crash the entire OpenSesame experiment mid-session. Documenting this specific launch sequence prevents data loss. Wow, that's a level of detail born from experience, I bet. What about the participant setup for EEG? Also highly detailed. The protocol included steps like measure the participant's head circumference, select the right cap size, fit the cap, apply gel, but then a really specific point related to the hardware. To prevent the cable bundle and splitter box from pulling on the EEG cap during the experiment, which can create noise artifacts, the protocol instructed the experimenter to securely clip the splitter box onto a towel draped over the participant's upper back or shoulders. [Ah], clever. Using a towel as an anchor point. Exactly. A very practical, physical instruction crucial for high-quality data documented explicitly. They even had a protocol for failures, right? For when a session inevitably crashes or has to be stopped early. Yes, foresight for dealing with the unexpected. If a session terminated prematurely, the experimenter had a clear, documented procedure. Step one, note the incident and the last completed task in that online logbook. Okay. Step two, and this is critical, immediately go into the data folder and rename the log file generated by that incomplete first run. Append something like first run to the file name. Why rename it? To prevent the next step, restarting the session, from simply overwriting the partial data from the first attempt. You preserve the record of what did happen. Got it. So rename, then restart. Then restart the session, usually from the beginning of the task that was interrupted or the next one. Only much later, during data processing, would you carefully merge the data, making sure to only include fully completed tasks, and discarding the partial run data. This preserves data integrity. That's incredibly thorough. I also like the detail about the subtle cues built into the experiment for the experimenter, not the participant. Yes, the screens with the orange squares or stripes. That was quite ingenious. Tell us about those. So, at certain points during the experiment, perhaps between blocks, a screen would appear that looked unremarkable to the participant but contained these visual cues, maybe orange shapes, maybe faint letters like ISR. And ISR stands for? Likely impedance. Signal recording. These screens we're silent signals to the experimenter, who should be monitoring nearby, to perform a necessary check. Maybe check the electrode impedance levels on the EEG computer, confirm the signal quality, or verify that the recording software is actually saving data. So it prompts a check without alerting or distracting the participant. Precisely. And the way to move past these check screens was also designed for control. How so? The experimenter had to press the letter C on their keyboard twice to make the experiment continue. Crucially, the instruction press C to continue was not displayed on the screen to the participant. [Ah], a hidden command only the experimenter knows. Exactly, a double lock. This ensures the experiment only proceeds after the experimenter has actively performed their check and confirmed readiness by pressing C twice. It provides essential experimenter control over the session flow without adding any confounding instructions or delays for the participant. Documenting these human workflow steps is just as important as documenting the code. So taking this all together, this deep dive really highlights how a structured, meticulously documented, open-source approach, like the one developed by [SURNAME] and colleagues using R and OpenSesame, can fundamentally elevate the reliability of research, especially in complex fields like cognitive science. It really does. It delivers what you call the reliability dividend. When materials are truly fair, findable, accessible, interoperable, reusable, and the workflow that created them is transparent and testable, the research itself becomes a much more solid foundation to build upon. It moves beyond just fixing bugs in the short term. And builds in quality control and efficiency for the long term. The research materials become genuine scientific assets, not just disposable outputs. In thinking about the bigger picture, this connects directly to the responsible use of resources in science, doesn't it? These kinds of studies, especially involving things like EEG, are expensive. They take a lot of time, participant effort, researcher effort, grant money. Huge investments. And if all that effort rests on data derived from stimuli that we're created using some opaque, untraceable, potentially error-ridden ad hoc method, well, the value of that entire investment is questionable, isn't it? The whole edifice is shaky. Precisely. So adopting these kinds of best practices, aiming for that place, high level of reproducibility via modular, well-documented open workflows isn't just about methodological neatness. It's arguably about making the best possible use of limited scientific resources and ultimately increasing the trustworthiness and reliability of the entire scientific endeavor. It sets a benchmark. It really does. So perhaps a final thought for you, our listener, to take away from this. Think about your own work, your own knowledge management, even for personal projects or notes. How could applying some of these FAIR principles, making things findable, accessible, interoperable, reusable, help your future self? Could structuring your notes, your code, your project [Mm]-[hmm]. Knowledge and insights remain understandable usable and maybe even shareable later on it's that idea of investing a bit more effort upfront to reap significant benefits in clarity and usability later protecting your future self from the curse of knowledge. A worthwhile investment indeed. Thank you so much for joining us for this deep dive into making research truly fair. My pleasure. It's a crucial topic. Until next time. Farewell, everyone.

