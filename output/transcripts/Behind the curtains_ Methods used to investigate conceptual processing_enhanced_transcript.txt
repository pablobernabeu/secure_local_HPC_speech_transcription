Behind the curtains_ Methods used to investigate conceptual processing_enhanced
===============================================================================

Input file: Behind the curtains_ Methods used to investigate conceptual processing_enhanced.wav
Processed: 2025-11-04 17:17:29
Model: openai/whisper-large-v3
Language: English
Edited to remove likely spurious repetitions.
Privacy: Personal names masked using internal list (review recommended)
===============================================================================

Welcome back to The Deep Dive. This is where we sift through stacks of research, articles, and new findings to bring you the most important insights, all tailored for you, our really curious listener. Great to be back. So in our previous deep dives, we've really explored the what and the why of how we make meaning in our minds. We've talked concepts, the big theories, why it's also fundamental to being human. That's right. We've covered some fascinating ground, conceptually speaking, thinking about how our minds actually build meaning. But today, we're shifting focus a bit. We're moving from those big, maybe abstract questions to the really practical, concrete how. Our mission today is really to unpack the specific scientific tools, the methods that researchers actually use day to day to investigate conceptual processing. It's about bridging that gap, you know, between asking an abstract question about thought and turning it into something, well, something measurable, something you can collect data on and analyze. And that sounds like a huge challenge, right? Because understanding a word, it feels instant, but it's happening in tiny fractions of a second. So the tools have to be incredibly precise. How hard is it to actually capture something that fast? Oh, it's incredibly challenging. And that need for precision, it really can't be overstated. I mean, think about it. The meaning of a word can start to activate in your brain within like 100 or 200 milliseconds, maybe even faster for some aspects. Wow. So if your methods aren't sensitive enough to capture things happening on that kind of time scale, you're basically missing the crucial cognitive action. So yeah, today we're going on a detailed tour of the Modern Psycholinguists Tool toolkit. We'll look at the experimental designs, the neurocognitive instruments that let us sort of peek inside the brain in real time, and also the statistical techniques needed to make sense of all that data, which can be pretty messy. It sounds like a fascinating look behind the scientific curtain. And I understand, to make this more concrete, we're going to use a specific example throughout. Yes, exactly. To ground this discussion, we'll refer back to a 2022 PhD thesis by [NAME] [SURNAME]. His work is a great example because it really employed many of these contemporary methods quite effectively. It gives us a practical anchor. Perfect. So where do we start with the foundational experiments, the ones based on just observing behavior? Let's start right there. With a behavioral toolkit, those clever experiments designed to capture cognitive events just by measuring responses like button presses. Okay, so let's say we want to measure the most basic thing. How quickly someone just recognizes that a string of letters is a real word. What's the absolute classic task for that? The go-to method. Right. The gold standard for that specific question is definitely the lexical decision task or LDT. It's been a cornerstone of psycholinguistics since, well, the 1970s, really. And it's power lies in it's simplicity. Basically, you have a participant sitting at a computer. On the screen, a string of letters appears. It could be a real word like, say, girl, or it could be a complete non-word, something like XLFFE. Their job is just to press one button as fast as they can if it's a real word, and a different button if it's not a word. Speed and accuracy are key. Simple enough. So what data do you get from that button press? And what does it tell us about the mind's dictionary? You're primarily measuring two key things. Reaction time, literally how many milliseconds it takes them to press the button and accuracy did they get it right what these tell us primarily is about the speed of lexical access lexical access meaning how quickly your brain can sort of find and retrieve that word's entry in your mental dictionary, your lexicon. And one of the most robust findings from decades of LDT research is the frequency effect. [Ah], right. Common words are faster. Exactly. High-frequency words, words you encounter all the time like table, consistently get faster reaction times than low-frequency words, things like pulpit. It suggests that common words have a more easily accessible, maybe more strongly represented entry in our minds. It's a direct window into how usage shapes our mental architecture. That makes intuitive sense. The brain optimizes for what's common. Now, you mentioned non-words like XL, F, FE. Does it matter how those non-words are constructed? Are random letters always best? That's actually a really important methodological point. It matters quite a bit. Those random strings, like XLFFE, are what we call orthographically illegal. They violate the spelling rules of English. So they're easy to spot. Very easy to reject, yeah. Yeah. Your brain pretty quickly says, nope, doesn't follow the rules. But researchers often use a different kind of non-word, which is arguably more informative. Pseudo. Pseudo words. Like fake words that could be real. Exactly. Think of things like trud or blutter. They are pronounceable and they do follow English spelling patterns, but they just don't happen to have a meaning assigned to them. Okay. And why are those more useful than just random letters? Well, because they feel much more word-like. They're harder to reject. Your brain can't just dismiss them based on illegal letter combinations. It has to engage more deeply. [Ah], so it forces more processing. Precisely. It engages orthographic processing, checking the visual form, and probably phonological processing too, sort of sounding it out internally. Rejecting a pseudoword requires a more thorough check against your mental lexicon. This gives us a cleaner signal about those specific recognition processes, not just a simple rule violation check. So using pseudowords really pushes the system, giving you a better look at actual word recognition mechanisms. So the LDT is fantastic for measuring recognition speed, that lexical access time. But I'm guessing it has limits if you want to know if someone actually processed the meaning of the word right. You've hit the nail on the head. That's it's primary limitation. Participants can often get the right answer on an LDT press the word button for table without necessarily diving deep into what table actually means. How so? Well, the decision might be based on a more superficial feeling of familiarity. You know, does this string look like a word I know? Some computational models, like the diffusion model, even suggest the decision is based on accumulating enough wordness evidence over time. It might reach a threshold for word before full semantic retrieval happens. So it's more about form than meaning, potentially. It can be, yes. It doesn't guarantee deep semantic engagement. So if your research question is specifically about how meaning is processed, the LDT on it's own might not be the right tool. You need something else. Okay, so what's the alternative if you absolutely need to ensure participants are thinking about the meaning? For that, researchers often employ the Semantic Decision Task, or SDT. The whole point of the SDT is to force participants to make a judgment based explicitly on some aspect of the word's meaning. Can you give an example? Sure. A classic example comes from the Calgary Semantic Decision Project. Participants saw a word like hammer or justice and had to decide, again, quickly, if the word's meaning was primarily concrete, something tangible, perceivable, or abstract, a concept, an idea. So you have to access the meaning to answer. Exactly. You can't do the task without considering what the word represents. Other versions might ask if something is living or not living, an animal or an object, big or small. The common element is the decision hinges on semantic properties. And what does this reveal that the LDT doesn't? What kinds of findings emerge? Well, it gives you a much more direct window into how conceptual information is structured and accessed. For instance, with that concrete, abstract task, a consistent finding is that reaction times are fastest for words at the clear ends of the spectrum. Like rock or truth? Precisely. Rock is super concrete. Truth is super abstract, easy calls, fast responses. But words that fall somewhere in the middle, maybe words that have both concrete and abstract aspects, or are just generally more ambiguous on that dimension. They take longer. Significantly longer, yes. And that pattern tells you something important about the graded nature of these conceptual dimensions. It's not just a binary switch. It's a continuum. And navigating the fuzzy middle ground takes more cognitive effort. It shows how concepts are represented, not just accessed. And referencing back, Bernabeu's thesis used an SDT precisely because the research questions we're about how participants use semantic information. It ensured they we're tapping into meaning. OK, so we have tasks for recognition speed and tasks for accessing meaning. What about understanding the connections between words and our minds? How are mental dictionary is wired together? [Ah], now you're talking about the semantic priming paradigm. This is another foundational technique, and it often builds directly on the LDT, but adds a clever twist. Which is? Before the main word you're judging the target word, let's say nurse, another word, the prime, is flashed very briefly on the screen. Right. And the classic finding here is really robust, isn't it? If the prime is related, like doctor, then people are significantly faster to recognize nurse as a word compared to when the prime is unrelated, like say bread. It's like doctor gives nurse a little boost. That's a great way to put it. The dominant explanation for this is a concept called spreading activation. Imagine your mental lexicon isn't just a list, but a huge interconnected network. Each word is a node, and related words have connections between them. Like a spider web of concepts. Exactly. When you see or hear the prime word doctor, it's node in this network becomes activated. But that activation doesn't just stay put. It automatically sort of spreads out along the connections to closely related nodes, nurse, hospital, stethoscope, whatever is strongly linked. So those related words get a head start. Precisely. They're already partially activated, pre-warmed, if you like, so when the target word nurse appears, it takes less time for it's activation to reach the threshold needed for recognition. It reveals the associative structure of our minds, how activating one concept primes others. Now, in these priming studies, there's a really critical timing variable, isn't there? The SOA, stimulus onset asynchrony. Yes, the SOA. This is absolutely key methodologically. SOA is simply the time interval between the moment the prime word appears, it's onset, and the moment the target word appears, it's onset. And why is manipulating this time gap so important? What does it let researchers figure out? It's real power lies in helping us disentangle automatic versus more controlled strategic mental processes. This is where it gets really insightful. If you use a very short SOA, typically anything under about 500 milliseconds, maybe even 200 or 300. That's incredibly fast. Barely perceptible. Right. There's simply not enough time for the participant to consciously think, OK, I saw a doctor. Maybe the next word will be nurse. I'll prepare for that. Yeah. Any priming effect you see at that short SOA is not to reflect that fast, automatic, unconscious spread of activation through the semantic network. It's a pure measure of the brain's hardwired associations. So you're capturing the brain's immediate automatic reaction. Exactly. But now, contrast that with a long SOA. Let's say the prime stays on screen for 700 milliseconds for a second, or even longer, before the target appears. Plenty of time to think. Plenty of time. Now, the participant can engage in slower, deliberate, conscious strategies. They might see doctor and actively think of related words, generating expectations. So the priming effect you observe at a long SOA is likely a mixture. It includes that initial automatic spread, but it's potentially contaminated or boosted by these controlled attentional processes like prediction or strategic guessing. So by systematically varying the SOA, you can almost create a timeline of cognitive processing. That's precisely the idea. You can map out how effects change over time. Does the priming effect emerge quickly and automatically? Does it get stronger or weaker with more time for strategic thought? It lets you dissociate these different types of processing. And again, bringing it back to Bernabeu's work, manipulating SOA is a key part of this methodology to examine how the influence of different types of word information changed as processing unfolded over time. Okay, this overview of behavioral methods is fantastic. LDT, SDTT priming. They tell us a lot by inference from reaction times, but as you said, they are indirect. They measure the output, not the process itself inside the brain. So let's move to the tools that offer a more direct look. Where do we start if we want really precise timing? For precise timing, the go-to technique is electroencephalography, or EEG. It's major strength is it's incredible temporal precision. What it involves is placing a cap, usually embedded with many electrodes, onto the participant's scalp. Like a high-tech swimming cap with wires. Sort of, yeah. And these electrodes measure the tiny electrical fields generated by the summed activity of large populations of neurons firing together in the brain. It's like listening to the overall electrical rhythm or hum of the brain's activity. And it's advantage is capturing things millisecond by millisecond? Exactly. It's temporal resolution is excellent, perfectly suited for tracking cognitive processes like language understanding that happen incredibly fast. Now, EEG records this continuous electrical activity. But to link it to specific events, researchers often use a technique to calculate event-related potentials, or ERPs. How does that work? You take the continuous EEG recording from many, many trials where the same type of event happened, say, seeing an unexpected word. You time lock the EEG signal to the onset of that word in each trial, and then you average all those segments together. Why average? Because the brain's background electrical activity is quite noisy. Averaging across many trials cancels out the random noise, allowing the small, consistent electrical response specifically related to that type of event to emerge from the background. And for meaning processing, the most famous ERP component is the N-400. The N-400. I've definitely a word that fit grammatically but made no sense, like he spread the warm bread with socks. Socks. Definitely unexpected. Right. And what they found was that the bizarre, unexpected word socks elicited a distinct brainwave pattern, a large negative-going voltage deflection that peaked around 400 milliseconds after the word appeared. Hence, N400 negative at 400 milliseconds. So the brain registered the N400 negative at 400 milliseconds. So the brain registered the anomaly electrically at 400 meters. Precisely. But what's really crucial, and maybe the key insight for you, is that the N400 isn't just an on-off signal for weirdness. It's amplitude. How big that negative spike is is the brain's, wait, what? Right. Reaction or it's effort to make sense of the input. That's a perfect way to think about it. It's a measure of semantic processing load or integration effort timed with millisecond precision incredibly powerful for timing meaning processing but EEG must have a downside right what's it's main limitation it does have a significant one. Poor spatial resolution. While EEG tells you when something happens very precisely, those electrical signals measured on the scalp get smeared and distorted as they pass through the skull and scalp tissues. [Ah], so you can't easily tell exactly where in the brain the signal originated? Exactly. Pinpointing the precise neural source of an EEG signal is very challenging. You know, when the orchestra played the note, but it's hard to tell exactly which instrument played it from outside the concert hall. So for the where question, you need a different technique. And that technique would be? That would be functional magnetic resonance imaging, or fMRI. This is the tool researchers turn to when the primary question is about location of brain activity. How does fMRI work? Is it also measuring electrical signals? No, it works on a completely different principle. FMRI uses powerful magnetic fields to detect changes in blood oxygen levels in the brain. This is known as the BOLD-E signal, blood oxygen level dependent signal. Okay, how does blood oxygen relate to brain activity? The underlying idea is that when a brain region becomes more active, it's neurons need more energy, and thus they demand more oxygen. To meet this demand, oxygenated blood flow increases to that specific area. FMRI picks up on the magnetic differences between oxygenated and deoxygenated blood. So the BOLD-D signal is an indirect measure of neural activity. It's tracking the brain's plumbing response to neural firing, not the firing itself. So if EEG's strength is timing, fMRI's strength must be location. Precisely. FMRI offers excellent spatial resolution. It can generate detailed 3D maps showing which brain areas are active. Often with pinpoint accuracy down to a few millimeters, you can really see the geography of brain function. But there's always a trade-off. What's the catch with fMRI? The catch is it's poor temporal resolution. That blood flow response, the BOLD-E signal, is sluggish. It takes several seconds to rise and fall after the neurons fire. So fMRI can't track the rapid millisecond by millisecond unfolding of cognitive processes like EEG can. It gives you a great picture of where things happen, average over a few seconds, but not the precise when. So using these fMRI maps, what have we learned about where meaning processing happens in the brain? Is there, like, a single meaning module? That's a great question, and the answer from fMRI is quite clear. No, there isn't one single spot. Instead, semantic processing seems to rely on a consistent, distributed network of brain regions, often called the semantic system. A network. Which areas are typically involved? Several key players consistently pop up. There's the left inferior frontal gyrus, often linked to selecting or retrieving semantic information, kind of like searching your mental dictionary. Then there are regions in the posterior temporal lobe thought to be important for storing the actual conceptual knowledge. And another key hub is the angular gyrus, which seems crucial for integrating different pieces of information together. So it's more like a team effort across different brain areas. Can you give an example of an fMRI study using this network knowledge, maybe something looking at ambiguity? Absolutely. Studies investigating semantic ambiguity are a perfect example. Researchers might present sentences with high ambiguity, like the shell was fired towards the tank, where both shell and tank have multiple meanings. Right. Could be an artillery shell and a military tank or a seashell and a water tank. Exactly. And they compare the brain activity for those sentences to activity for low-ambiguity sentences like, her secrets we're written in her diary. The consistent finding is that the high-ambiguity sentences provoke significantly more activation within that left frontotemporal semantic network we just discussed. So the network works harder when meaning is unclear. That's the interpretation. FMRI literally shows you the increased metabolic demand, the extra computational effort the brain exerts when it has to sort through multiple possible meanings and select the appropriate one based on context. It highlights the brain regions doing the heavy lifting for meaning resolution. Okay, so EEG for when, fMRI for where. What other major tools are in the neurocognitive toolkit? Is there something that connects cognition more directly to action, maybe? Yes, there is. A third major approach is eye tracking, particularly when used in what's called the visual worldigm, or VWP. This gives a unique window by linking language processing to where people look. Visual World Paradigm, how does that work? In a typical VWP setup, a participant looks at a visual display on a screen. This display usually contains several objects or pictures, maybe, for instance, a candle, a piece of candy, a carriage, and a sandwich. While they're looking, two things happen. A high-speed camera tracks their eye movements with incredible precision, and they listen to spoken sentences or instructions related to the display. And the key idea of the linking hypothesis is that where our eyes go reflects what our brain is processing from the speech almost immediately. That's the core assumption, and it holds up remarkably well. Our eye movements are very tightly time-locked to the language we hear so if someone hears the instruction pick up the candy their eyes will typically saccade make a rapid jump to the picture of the candy on the screen just as or sometimes even slightly before the word candy finishes being spoken. Wow. Even before the word finishes, that suggests prediction. Exactly. And that's the real power of the VWP. It reveals cognitive processes, especially prediction, that happen implicitly before any overt response like pressing a button or speaking. It shows the brain actively anticipating what's coming next. What's the classic example demonstrating this prediction? The classic one involves a scene, say, with a picture of a boy, a cake, a ball, and maybe a toy car. The participant starts hearing this sentence, the boy will eat thee. And their eyes jump to the cake. Very often, yes. Yeah. Even before the brain's prediction unfold in real time through eye movements. It really is. The VWP provides this direct observable trace of the brain operating proactively, constantly using context to anticipate upcoming information rather than just passively waiting for it. It's compelling evidence for the predictive nature of language comprehension. And does this method have any practical advantages, too, maybe in terms of who can participate in these studies. It definitely does. Yeah. Because the basic task is just looking and listening. It doesn't require complex button presses or even literacy. This makes the VWP incredibly valuable for studying language processing in populations where other methods might be difficult. Like young children. Exactly. Pre-linguistic infants, young children learning language, older adults who might have slower motor responses, or even patient populations with language impairments or motor difficulties. It opens doors to understanding language development and breakdown in ways other paradigms might not allow. It's simplicity is a huge strength. [NAME] Pletka- OK, so we've covered this amazing array of tools for collecting data reaction times, brain waves, eye movements. Researchers end up with these huge, rich data sets. But then comes the challenge, analyzing it all. Because as you mentioned, human responses are inherently variable, right? Any single reaction time isn't just about the experiment. It's also about the specific person and the specific word they saw. Absolutely. You've put your finger on a fundamental challenge in analyzing psycholinguistic data. There's always variability. The same person might respond slightly faster or slower on different trials, different people have different baseline speeds, and some words are just intrinsically easier or harder than others. This inherent variability, influenced by both participants and the specific items like words or sentences used in the experiment creates a complex data structure and this structure has a specific name doesn't it something about effects being crossed. That's right. It's known as a crossed random effect structure. Think about it. Each participant responds to multiple different experimental items, and each item is responded to by multiple different participants. The sources of variability participants and items are crossed with each other. Everyone sees multiple words. Every word is seen by multiple people. And traditional statistical methods, like the ones maybe you used decades ago, struggled with this kind of structure. They really did. For a long time, the standard approach was analysis of variance, or ANOVA. But standard ANOVA assumes independent observations, which you clearly don't have in this crossed structure. So researchers developed workarounds. Like what? The common workaround was a bit cumbersome. It involved conducting two separate ANOVAs. First, you'd average the data for each participant across all the items they saw and run an ANOVA on those participant averages that was called the F1 analysis. Then, you'd average the data for each item across all the participants who saw it and run a second ANOVIA on those item averages the F2 analysis. Two separate tests. That sounds complicated. It was. And a finding was only considered truly reliable if it reached statistical significance in both the F1 by participants and F2 by items analyses. What we're the downsides of that approach? There we're several significant drawbacks. Firstly, all that averaging throws away a huge amount of valuable trial-level data. You lose sensitivity because you're smoothing over the variability instead of modeling it. Secondly, it could lead to ambiguous results. What if your effect was significant in F1, but not F2 or vice versa? It wasn't an ideal solution. It was sort of forcing a square peg into a round hole. Okay, so that was the old way. What's the modern solution that handles this cross structure more effectively? The modern standard, and really a revolutionary change in the field, is the use of linear mixed effects models, often abbreviated as LMIMs. You might also hear them called multilevel models or hierarchical models. They are specifically designed to handle data with these kinds of nested or crossed dependencies. And how are they typically implemented? Is it complex software? They can be complex to fully understand, but the implementations become much more accessible thanks to powerful open source statistical software, especially the R programming language Okay, let's try to unpack how they work conceptually. You mentioned mixed effects. What does that mean? Maybe that crop science analogy you used earlier could help here. Let's bring back the crop scientist. Remember, they're testing a new fertilizer. The main question, does the fertilizer increase yield on average, relates to the fixed effect. This is the systematic experimental manipulation whose overall effect you want to estimate for the entire population, all farms in this case. In our language experiments, the fixed effect might be to semantic priming reduce reaction time on average, or is the N400 larger for anomalous words, on average? So the fixed effect is the main research question about the overall trend. What about the mixed part, the other effects? The mixed part comes from including random effects alongside the fixed effects. And this is where LMEMs really shine in handling that variability we talked about. They explicitly model the variation associated with the specific participants and specific items in your study. How do they model it? Primarily through two types of random effects. The first is random intercepts. Think back to the farms. Some farms are just naturally more fertile than others, right? They have a higher baseline yield regardless of the fertilizer. A random intercept in the model accounts for this by estimating a unique baseline level for each individual farm. So in a language study? In a language study, this means the model estimates a unique baseline reaction time for every single participant, because some people are just faster overall. And D, a unique baseline difficulty for every single word because some words are just easier. It statistically controls for these baseline differences instead of treating them as noise. That's a huge advantage. That makes sense. It accounts for the starting point for each person and each word. What's the other type of random effect? Slopes? Yes, random slopes. This takes it a step further. Sticking with the farms, maybe the fertilizer works really well on Farm A, giving a big yield boost, but it's less effective on farm B, maybe due to soil type. The size of the fertilizer effect itself might vary from farm to farm. Random slopes allow for this. So the effect isn't assumed to be identical for everyone or everything. Exactly. In a priming study, a random slope for the priming effect would allow the model to estimate a potentially different size of the priming effect for each individual participant, and maybe even for each individual target word. Some people might show a huge priming effect, others might show a smaller one. Random slopes capture this variation in the magnitude of the effect across participants or items. Wow. So MMs aren't just controlling for baseline differences. They're modeling how the effect itself might differ across individuals and items. That feels like a massive leap forward. It really is a conceptual shift. What used to be considered error variance in Anamufea, the fact that people and words differ is now explicitly incorporated into the model as a meaningful source of variation that we want to understand. It moves from just asking, is there an effect on average, to much more nuanced questions. Like what kinds of questions? Like how much does the priming effect vary across individuals? And even better, can we predict why some individuals show a larger effect than others? Maybe it relates to their vocabulary size or their working memory capacity. You can include these individual difference measures in the model to see if they explain some of the variation in the random slopes. That sounds incredibly powerful for getting a deeper understanding. And Bernabeu's thesis used these kinds of models. Yes, absolutely. The analyses in his thesis used L-memes, including complex models with interaction terms, testing precisely these kinds of questions, how the effects of word properties might be modulated by individual participant characteristics, all within a single, unified statistical framework. It's the state of the art for this kind of data. Okay, we've journeyed through designing experiments, collecting data with sophisticated tools, and analyzing it with powerful statistical models like L-MEMS. But there's one final, crucial piece of the methodological puzzle we need to discuss, ensuring the findings are actually trustworthy and replicable. This involves analyzing the experiment's own sensitivity, right? Yes, this is absolutely critical, especially in light of ongoing discussions about reproducibility in science. We need to know if our studies actually had a fair chance of detecting the effects we we're looking for. This boils down to the concept of statistical power. Statistical power. Can you define that for us? Sure. Formally, statistical power is the probability that a statistical test will correctly reject the null hypothesis, when the null hypothesis is indeed false. Put more simply, it's the probability of finding an effect if a real effect actually exists. It's the chance of avoiding a type two error or a false negative, concluding there's no effect when there really is one. So high power means you're likely to detect a real effect. Low power means you might miss it. Exactly. You want your study to have high sensitivity, high power, so you can be confident that if the effect you're interested in is real, your experiment is capable of detecting it. And power is closely tied to the number of participants, the sample size. Very closely tied. Detecting small, subtle effects often requires more statistical power, which usually translates directly into needing a larger sample size. If your study is underpowered, perhaps because the sample size is too small for the effect you're looking for, you run a high risk of getting a non-significant result, even if the effect is genuinely there. Like trying to see a faint star with binoculars that aren't powerful enough. Oh, so how do researchers figure out before they start collecting data how many participants they actually need to have decent power? That's done through what's called an a priori power analysis. A priori just means beforehand. This is a crucial step in the planning phase of a study. To do it, the researcher needs to specify a few key things. What are they? First, the desired level of power conventionally. This is often set at 80%, meaning an 80% chance of detecting a true effect. Second, the alpha level, the threshold for statistical significance, usually. 05. And third, this is the trickiest part, they need to specify the smallest effect size of interest, SO per I. Smallest effect size of interest. What does that mean? It means the minimum size of an effect that the researcher would consider scientifically meaningful or practically relevant. It's saying, if the true effect is smaller than this, I'm okay with potentially missing it, but I want an 80% chance of finding effects at least this big or larger. Once you specify those three things, power, alpha, and sessoi, the analysis calculates the minimum sample size inquired to achieve that. That sounds like a really important planning tool. What about after a study is done, maybe the result was null not significant is there a way to assess the study sensitivity then yes after the study you can perform a sensitivity analysis this asked a different question given the sample size I actually ended up using, what was the smallest effect size I could have reliably detected with 80% power? This provides a clear statement about the study's limitations. If the sensitivity analysis shows you could only detect very large effects, it helps interpret a null result. Maybe a smaller, meaningful effect was present, but the study just wasn't sensitive enough to find it. That's useful. Now, I've heard warnings about a certain type of power analysis done after the fact that's considered bad practice, something called post hoc power. [Ah], yes. You should absolutely avoid post hoc or sometimes called observed power analysis. Yeah. This is when someone gets a non-significant result, takes the small non-significant effect size they observed in their data and plugs that into a power calculation. Why is that bad? It's considered uninformative and circular. If your study failed to find a significant effect, your observed effect size will necessarily be small and associated with a high p-value. Calculating power based on this small, non-significant observed effect will always tell you that you had low power. It doesn't add any new information, it just restates the non-significant finding in terms of power. It's statistically redundant and often misleading. Focus on a priori power or sensitivity analysis based on the smallest effect size of interest, not the observed effect from a null result. Good to know. So for those complex linear mixed effects models we talked about, calculating power isn't straightforward with simple formulas. How do researchers determine sample size or sensitivity for LMIMs? That's right. The complexity of LMIMs, especially with random slopes and crossed effects, means simple formulas usually don't work. The standard approach here is simulation-based power analysis. Simulation-based? How does that work? Sounds intensive. It can be computationally intensive, yes, but the logic is pretty straightforward. Basically, you become the simulator of your own experiment. First, you build a computational model based on your theory, specifying the size of the fixed effects you expect, and importantly, the amount of variance you expect in your random intercepts and slopes. So you create a virtual reality of your expected results? Kind of, yes. Then, using this model, you generate thousands of simulated datasets, each with a specific sample size, say, N100. These datasets mimic what real data might look like if your theory and effect size estimates are correct. Third, you run your planned LMM analysis on each one of these thousands of simulated datasets. Finally, you just count. What proportion of those thousands of analyses correctly detected the effect you originally built into the simulation, I. E., yielded a statistically significant result for your fixed effective interest. That proportion is your estimated power for that specific sample size at effect size. You might run simulations for several different sample sizes to find the one that gives you your desired 80% power. It's a very thorough approach, and I understand the [SURNAME] thesis provides a really telling example of this in action. It provides an excellent and quite illuminating example. His simulation-based power analysis for the LMIMs he planned to use showed something interesting. To reliably detect the main effect, he was interested in a relatively straightforward effect of language-based information. A sample size of around 300 participants was sufficient to achieve adequate power. Okay, 300 sounds manageable for some labs, but. Yeah, but the analysis also looked at the power needed to detect more subtle interaction effects, specifically how individual differences like vocabulary size might modulate the effect of other word properties to reliably detect these more nuanced interaction effects with 80% power. How many did he need? The simulations indicated that more than 1, 000 participants we're necessary. Wow. Over 1 thousand. That's a huge difference. What are the implications of that finding? It has profound implications. It starkly illustrates that as our scientific questions become more sophisticated, as we move beyond asking about simple average effects and try to understand the complex interplay between different factors and individual differences, the sample size requirements can increase dramatically. Detecting interactions often requires much larger samples than detecting main effects. Which really pushes towards collaborative science. Absolutely. This is a major driver behind the move towards big team science initiatives and multi-lab collaborations in psychology and cognitive science. Often, answering these more nuanced but arguably more interesting questions is simply beyond the resources of a single lab. It requires pooling data and effort across many researchers to get the statistical power needed. So, we've really covered a lot of ground today. We've gone through this incredible toolkit, the behavioral workhorses like LDT, SDT, and semantic priming, the neurocognitive techniques giving us windows into timing with EEG and the N400, location with fMRI, and real-time prediction with eye tracking in the visual world paradigm. And then the sophisticated statistical tools like L-MEMS and the crucial underpinning of ensuring statistical power. It's quite an arsenal. It is indeed a sophisticated arsenal, and it's one that's constantly being refined and improved. Looking ahead, I think we'll see even more integration of these methods. For example, researchers are increasingly doing simultaneous EEG and fMRI, or combining EEG with eye tracking. To get the best of both worlds. Exactly. To get millisecond timing information alongside spatial localization, or timing alongside predictive eye movements. These multimodal approaches offer a richer, more complete picture of cognitive processes. And as we just discussed, the need for larger samples for complex questions is definitely driving more collaboration, more big team science. That makes sense. Are there other trends, maybe trying to make the experiments themselves feel more natural? Yes, there's definitely a push towards greater ecological validity, making lab tasks better reflect real-world language use. New technologies are helping here. Virtual reality, or VR for instance, is starting to be explored. VR for language research. How? Well, VR allows researchers to immerse participants in much more complex, dynamic and interactive environments compared to just looking at words on a screen while still maintaining a high degree of experimental control. Imagine studying conversation in a simulated social setting or navigation instructions in a virtual town. It's about capturing language processing in contexts that are closer to our everyday experience. That sounds incredibly exciting. It really feels like the quest to understand how we process meaning is constantly evolving. It absolutely is. Understanding how the brain creates meaning is one of the deepest questions we can ask about ourselves. We're certainly far from having all the answers, but the field now possesses this really well-developed, rigorous, and constantly improving set of methods to tackle those questions systematically. The journey continues, but we have better maps and tools than ever before. And perhaps the final thought for you, our listener, is that this whole process we've discussed today, the careful design, the precise measurement, the sophisticated analysis, the critical evaluation of sensitivity, the constant refinement of methods, this isn't just about finding answers. It's the very engine of scientific progress itself. It's how we push the boundaries, how we build reliable knowledge, and how we slowly unravel the incredible complexity of the human mind. It's the unseen foundation beneath every discovery. I want to thank my expert guide for navigating us through this complex landscape today. My pleasure. It was a great discussion. And thank you, as always, for joining us. We hope you'll dive deep with us again next time on The Deep Dive.

